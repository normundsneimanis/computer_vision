{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-family:Georgia; font-size:2.5vw; color:lightblue; font-weight:normal; text-align:center; background:url('./Animations/Title Background.gif') no-repeat center; background-size:cover)\">\n",
    "<br><br>\n",
    "基于定向FAST和旋转BRIEF的ORB（Oriented FAST and Rotated BRIEF）算法\n",
    "<br><br><br>\n",
    "</div>\n",
    "\n",
    "# 简介\n",
    "\n",
    "计算机视觉中最具挑战性的问题之一是物体检测。对象检测是识别图像中的特定对象并能够确定图像内这些对象的位置的能力。例如，如果我们在下面的图像中执行汽车检测，我们不仅要说出图像中有多少辆汽车，还要确定这些汽车在图像中的*位置*。\n",
    "<br>\n",
    "<figure>\n",
    "  <img src = \"./Animations/cars.jpg\" width = 80% style = \"border: thin silver solid; padding: 10px\">\n",
    "      <figcaption style = \"text-align: center; font-style: italic\">Fig1. - Car Detection.</figcaption>\n",
    "</figure> \n",
    "<br>\n",
    "为了执行这种基于对象的图像分析，我们将使用ORB。 ORB是一种非常快速的算法，可以根据检测到的关键点创建特征向量。之前，你已经了解到ORB具有一些很好的属性，例如旋转、光照变化不变性以及抗噪性。\n",
    "\n",
    "在这个notebook中，我们要运行并实现ORB算法，使用人脸关键点在图像中检测人脸，在这个过程中，你会看到这些属性。\n",
    "\n",
    "### 加载图像并导入资源\n",
    "\n",
    "在下面的代码中，我们将使用OpenCV来加载一张女性人脸的图像，用作我们的训练图像。在这里，`cv2.imread()`函数会将图像加载为BGR，我们要把该图像转换为RGB，因此我们可以使用正确的颜色显示它。与之前一样，我们要将BGR图像转换为灰度图像进行分析。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the default figure size\n",
    "plt.rcParams['figure.figsize'] = [20,10]\n",
    "\n",
    "# Load the training image\n",
    "image = cv2.imread('./images/face.jpeg')\n",
    "\n",
    "# Convert the training image to RGB\n",
    "training_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Convert the training image to gray Scale\n",
    "training_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Display the images\n",
    "plt.subplot(121)\n",
    "plt.title('Original Training Image')\n",
    "plt.imshow(training_image)\n",
    "plt.subplot(122)\n",
    "plt.title('Gray Scale Training Image')\n",
    "plt.imshow(training_gray, cmap = 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定位关键点\n",
    "\n",
    "ORB算法的第一步是定位训练图像中的所有关键点。找到关键点后，ORB会创建相应的二进制特征向量，并在ORB描述子中将它们组合在一起。\n",
    "\n",
    "我们要使用OpenCV的`ORB`类来定位关键点并创建相应的ORB描述子。另外，要使用`ORB_create()`函数设置ORB算法的参数。 `ORB_create()`函数的参数及其默认值如下：\n",
    "\n",
    "\n",
    "`cv2.ORB_create(nfeatures = 500,\n",
    "               scaleFactor = 1.2,\n",
    "    \t       nlevels = 8,\n",
    "    \t       edgeThreshold = 31,\n",
    "    \t       firstLevel = 0,\n",
    "    \t       WTA_K = 2,\n",
    "    \t       scoreType = HARRIS_SCORE,\n",
    "    \t       patchSize = 31,\n",
    "    \t       fastThreshold = 20)`\n",
    "\n",
    "参数：\n",
    "\n",
    "* **nfeatures** - *int*  \n",
    "  确定想要定位的特征（即关键点）的最大数量。\n",
    "\n",
    "\n",
    "* **scaleFactor** - *float*  \n",
    "  金字塔抽取比率必须大于1。ORB会使用图像金字塔来查找特征，因此必须提供金字塔中每个层与金字塔所具有的级别数之间的比例因子。 `scaleFactor = 2`表示经典金字塔，其中每个下一级别的像素比前一级少4倍。大比例因子将会减少检测到的特征数量。\n",
    "\n",
    "\n",
    "* **nlevels** - *int*  \n",
    "  金字塔等级的数量。最小级别的线性大小等于input_image_linear_size / pow（scaleFactor，nlevels）。\n",
    "\n",
    "\n",
    "* **edgeThreshold** - - *int*  \n",
    "  未检测到特征的边缘大小。由于关键点具有特定的像素大小，因此必须从搜索中排除图像的边缘。 `edgeThreshold`的大小应等于或大于patchSize参数。\n",
    "\n",
    "\n",
    "* **firstLevel** - *int*  \n",
    "  此参数用于确定应将哪个级别当做金字塔中的第一级别。它在当前实现中应为0。通常情况下，具有统一标度的金字塔等级被认为是第一级。\n",
    "\n",
    "\n",
    "* **WTA_K** - *int*  \n",
    "  用于生成定向BRIEF描述子的每个元素的随机像素的数量。可能的值为2、3和4，其中2为默认值。例如，值3意味着一次选择三个随机像素来比较它们的亮度，并返回最亮像素的索引。由于有3个像素，因此返回的索引将为0、1或2。\n",
    "\n",
    "\n",
    "* **scoreType** - *int*  \n",
    "  此参数可以设置为HARRIS_SCORE或FAST_SCORE。默认的HARRIS_SCORE表示Harris角点算法用于对特征进行排名。该分数仅用于保留最佳特征。 FAST_SCORE生成的关键点稳定性稍差，但计算起来要快一些。\n",
    "\n",
    "\n",
    "* **patchSize** - *int*  \n",
    "  定向BRIEF描述子使用的补丁的大小。在较小的金字塔层级上，由特征覆盖的感知图像区域将更大。\n",
    "\n",
    "\n",
    "我们可以看到，`cv2. ORB_create()`函数支持各种参数。前两个参数（`nfeatures` and ` scaleFactor`）最有可能需要更改。其他参数只要保留其默认值，就可以获得不错的结果。\n",
    "\n",
    "在下面的代码中，我们将使用`ORB_create()`函数将我们想要检测的关键点的最大数量设置为200，并将金字塔抽取比率设置为2.1。然后，使用` .detectAndCompute (image)`方法定位给定训练`image`中的关键点并计算其对应的ORB描述子。最后，使用` cv2.drawKeypoints()`函数把ORB算法找到的关键点可视化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import copy to make copies of the training image\n",
    "import copy\n",
    "\n",
    "# Set the default figure size\n",
    "plt.rcParams['figure.figsize'] = [14.0, 7.0]\n",
    "\n",
    "# Set the parameters of the ORB algorithm by specifying the maximum number of keypoints to locate and\n",
    "# the pyramid decimation ratio\n",
    "orb = cv2.ORB_create(200, 2.0)\n",
    "\n",
    "# Find the keypoints in the gray scale training image and compute their ORB descriptor.\n",
    "# The None parameter is needed to indicate that we are not using a mask.\n",
    "keypoints, descriptor = orb.detectAndCompute(training_gray, None)\n",
    "\n",
    "# Create copies of the training image to draw our keypoints on\n",
    "keyp_without_size = copy.copy(training_image)\n",
    "keyp_with_size = copy.copy(training_image)\n",
    "\n",
    "# Draw the keypoints without size or orientation on one copy of the training image \n",
    "cv2.drawKeypoints(training_image, keypoints, keyp_without_size, color = (0, 255, 0))\n",
    "\n",
    "# Draw the keypoints with size and orientation on the other copy of the training image\n",
    "cv2.drawKeypoints(training_image, keypoints, keyp_with_size, flags = cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "# Display the image with the keypoints without size or orientation\n",
    "plt.subplot(121)\n",
    "plt.title('Keypoints Without Size or Orientation')\n",
    "plt.imshow(keyp_without_size)\n",
    "\n",
    "# Display the image with the keypoints with size and orientation\n",
    "plt.subplot(122)\n",
    "plt.title('Keypoints With Size and Orientation')\n",
    "plt.imshow(keyp_with_size)\n",
    "plt.show()\n",
    "\n",
    "# Print the number of keypoints detected\n",
    "print(\"\\nNumber of keypoints Detected: \", len(keypoints))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正如右图所示，每个关键点都有一个中心、一个大小和一个角度。这个中心会确定图像中每个关键点的位置，每个关键点的大小由BRIEF用来创建其特征向量的补丁大小决定，而角度会告诉我们由rBRIEF确定的关键点的方向。\n",
    "\n",
    "找到训练图像的关键点并且计算了它们相应的ORB描述子之后，就可以对查询图像进行相同的操作。为了更清楚地查看ORB算法的属性，在下一部分中，我们将使用与我们的训练和查询图像相同的图像。\n",
    "\n",
    "# 特征匹配\n",
    "\n",
    "获得了训练*和*查询图像的ORB描述子之后，最后一步就是使用相应的ORB描述子在两个图像之间进行关键点匹配。这种*匹配*通常由匹配函数执行。最常用的匹配函数是*Brute-Force*。\n",
    "\n",
    "在下面的代码中，我们将使用OpenCV的`BFMatcher `类来比较训练和查询图像中的关键点。使用`cv2.BFMatcher()`函数设置Brute-Force匹配程序的参数。 `cv2.BFMatcher()`函数的参数及其默认值如下：\n",
    "\n",
    "\n",
    "\n",
    "`cv2.BFMatcher(normType = cv2.NORM_L2,\n",
    "\t\t \t  crossCheck = false)`\n",
    "\n",
    "参数：\n",
    "\n",
    "* **normType**  \n",
    "  确定用于确定匹配质量的度量标准。默认情况下，`normType = cv2.NORM_L2`，它用于测量两个描述子之间的距离。但是，对于像ORB创建的二进制描述子一样，汉明度量更合适。汉明度量通过计算二进制描述子之间的不相似位的数量来确定距离。当使用`WTA_K = 2`创建ORB描述子时，要选择两个随机像素并在亮度上进行比较。最亮像素的索引返回为0或1。此类输出仅占用1位，因此应使用` cv2.NORM_HAMMING` 度量。另一方面，如果使用`WTA_K = 3`创建ORB描述子，则选择三个随机像素并在亮度上进行比较。最亮像素的索引返回0、1或2。这样的输出将占用2位，因此应该使用汉明距离的特殊变体，即`cv2.NORM_HAMMING2`（2代表2位）。然后，对于所选择的任何度量，当比较训练和查询图像中的关键点时，具有较小度量（它们之间的距离）的对被认为是最佳匹配。\n",
    "\n",
    "\n",
    "* **crossCheck** - *bool* \n",
    "  布尔变量，可以设置为`True` 或 `False`。交叉验证对于消除错误匹配非常有用。交叉验证通过执行两次匹配过程来完成。第一次匹配中，将训练图像中的关键点与查询图像中的关键点进行比较；第二次匹配中，将查询图像中的关键点与训练图像中的关键点进行比较（*即*，反向进行比较）。启用交叉验证时，只有当训练图像中的关键点*A*是查询图像中关键点*B*的最佳匹配时，该匹配才被视为有效，反之亦然（即，如果查询图像中的关键点*B* 是训练图像中的关键点*A*，该匹配则是最佳匹配）。\n",
    "\n",
    "设置了BFMatcher的参数之后，我们就可以使用`.match(descriptors_train, descriptors_query)`方法，使用它们的ORB描述子找到训练和查询图像之间的匹配关键点。最后，我们将使用` cv2.drawMatches ()`函数来可视化Brute-Force匹配程序找到的匹配关键点。此函数会水平堆叠训练和查询图像，并将训练图像中关键点的线条绘制到查询图像中对应的最佳匹配关键点。在这里，请注意，为了更清楚地查看ORB算法的属性，在以下示例中，我们将使用与我们的训练和查询图像相同的图像。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the default figure size\n",
    "plt.rcParams['figure.figsize'] = [14.0, 7.0]\n",
    "\n",
    "# Load the training image\n",
    "image1 = cv2.imread('./images/face.jpeg')\n",
    "\n",
    "# Load the query image\n",
    "image2 = cv2.imread('./images/face.jpeg')\n",
    "\n",
    "# Convert the training image to RGB\n",
    "training_image = cv2.cvtColor(image1, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Convert the query image to RGB\n",
    "query_image = cv2.cvtColor(image2, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Display the training and query images\n",
    "plt.subplot(121)\n",
    "plt.title('Training Image')\n",
    "plt.imshow(training_image)\n",
    "plt.subplot(122)\n",
    "plt.title('Query Image')\n",
    "plt.imshow(query_image)\n",
    "plt.show()\n",
    "\n",
    "# Convert the training image to gray scale\n",
    "training_gray = cv2.cvtColor(training_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Convert the query image to gray scale\n",
    "query_gray = cv2.cvtColor(query_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Set the parameters of the ORB algorithm by specifying the maximum number of keypoints to locate and\n",
    "# the pyramid decimation ratio\n",
    "orb = cv2.ORB_create(1000, 2.0)\n",
    "\n",
    "# Find the keypoints in the gray scale training and query images and compute their ORB descriptor.\n",
    "# The None parameter is needed to indicate that we are not using a mask in either case.\n",
    "keypoints_train, descriptors_train = orb.detectAndCompute(training_gray, None)\n",
    "keypoints_query, descriptors_query = orb.detectAndCompute(query_gray, None)\n",
    "\n",
    "# Create a Brute Force Matcher object. Set crossCheck to True so that the BFMatcher will only return consistent\n",
    "# pairs. Such technique usually produces best results with minimal number of outliers when there are enough matches.\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck = True)\n",
    "\n",
    "# Perform the matching between the ORB descriptors of the training image and the query image\n",
    "matches = bf.match(descriptors_train, descriptors_query)\n",
    "\n",
    "# The matches with shorter distance are the ones we want. So, we sort the matches according to distance\n",
    "matches = sorted(matches, key = lambda x : x.distance)\n",
    "\n",
    "# Connect the keypoints in the training image with their best matching keypoints in the query image.\n",
    "# The best matches correspond to the first elements in the sorted matches list, since they are the ones\n",
    "# with the shorter distance. We draw the first 300 mathces and use flags = 2 to plot the matching keypoints\n",
    "# without size or orientation.\n",
    "result = cv2.drawMatches(training_gray, keypoints_train, query_gray, keypoints_query, matches[:300], query_gray, flags = 2)\n",
    "\n",
    "# Display the best matching points\n",
    "plt.title('Best Matching Points')\n",
    "plt.imshow(result)\n",
    "plt.show()\n",
    "\n",
    "# Print the number of keypoints detected in the training image\n",
    "print(\"Number of Keypoints Detected In The Training Image: \", len(keypoints_train))\n",
    "\n",
    "# Print the number of keypoints detected in the query image\n",
    "print(\"Number of Keypoints Detected In The Query Image: \", len(keypoints_query))\n",
    "\n",
    "# Print total number of matching points between the training and query images\n",
    "print(\"\\nNumber of Matching Keypoints Between The Training and Query Images: \", len(matches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上面的示例中，由于训练图像和查询图像完全相同，因此，我们希望在两个图像中找到相同数量的关键点，并且所有关键点都匹配。我们可以清楚地看到，事实确实如此，ORB在两个图像中都找到了相同数量的关键点，而且Brute-Force匹配程序能够正确匹配训练和查询图像中的所有关键点。\n",
    "\n",
    "# ORB的主要属性\n",
    "\n",
    "我们现在将依次了解ORB算法的几大主要属性：\n",
    "\n",
    "* 尺度不变性\n",
    "* 旋转不变性\n",
    "* 光照不变性\n",
    "* 抗噪性\n",
    "\n",
    "同样，为了更清楚地看到ORB算法的属性，在下面的示例中，我们将使用与我们的训练和查询图像相同的图像。\n",
    "\n",
    "\n",
    "## 尺度不变性\n",
    "\n",
    " ORB算法具有尺度不变性。这意味着无论图像大小如何，它能够检测图像中的对象。为了看清楚这一点，我们现在将使用Brute-Force匹配程序来匹配训练图像和一张查询图像种的点，其中，该查询图像的尺寸是原始训练图像的1/4。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the default figure size\n",
    "plt.rcParams['figure.figsize'] = [14.0, 7.0]\n",
    "\n",
    "# Load the training image\n",
    "image1 = cv2.imread('./images/face.jpeg')\n",
    "\n",
    "# Load the query image\n",
    "image2 = cv2.imread('./images/faceQS.png')\n",
    "\n",
    "# Convert the training image to RGB\n",
    "training_image = cv2.cvtColor(image1, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Convert the query image to RGB\n",
    "query_image = cv2.cvtColor(image2, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Display the images\n",
    "plt.subplot(121)\n",
    "plt.title('Training Image')\n",
    "plt.imshow(training_image)\n",
    "plt.subplot(122)\n",
    "plt.title('Query Image')\n",
    "plt.imshow(query_image)\n",
    "plt.show()\n",
    "\n",
    "# Convert the training image to gray scale\n",
    "training_gray = cv2.cvtColor(training_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Convert the query image to gray scale\n",
    "query_gray = cv2.cvtColor(query_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Set the parameters of the ORB algorithm by specifying the maximum number of keypoints to locate and\n",
    "# the pyramid decimation ratio\n",
    "orb = cv2.ORB_create(1000, 2.0)\n",
    "\n",
    "# Find the keypoints in the gray scale training and query images and compute their ORB descriptor.\n",
    "# The None parameter is needed to indicate that we are not using a mask in either case.\n",
    "keypoints_train, descriptors_train = orb.detectAndCompute(training_gray, None)\n",
    "keypoints_query, descriptors_query = orb.detectAndCompute(query_gray, None)\n",
    "\n",
    "# Create a Brute Force Matcher object. Set crossCheck to True so that the BFMatcher will only return consistent\n",
    "# pairs. Such technique usually produces best results with minimal number of outliers when there are enough matches.\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck = True)\n",
    "\n",
    "# Perform the matching between the ORB descriptors of the training image and the query image\n",
    "matches = bf.match(descriptors_train, descriptors_query)\n",
    "\n",
    "# The matches with shorter distance are the ones we want. So, we sort the matches according to distance\n",
    "matches = sorted(matches, key = lambda x : x.distance)\n",
    "\n",
    "# Connect the keypoints in the training image with their best matching keypoints in the query image.\n",
    "# The best matches correspond to the first elements in the sorted matches list, since they are the ones\n",
    "# with the shorter distance. We draw the first 30 mathces and use flags = 2 to plot the matching keypoints\n",
    "# without size or orientation.\n",
    "result = cv2.drawMatches(training_gray, keypoints_train, query_gray, keypoints_query, matches[:30], query_gray, flags = 2)\n",
    "\n",
    "# Display the best matching points\n",
    "plt.title('Best Matching Points')\n",
    "plt.imshow(result)\n",
    "plt.show()\n",
    "\n",
    "# Print the shape of the training image\n",
    "print('\\nThe Training Image has shape:', training_gray.shape)\n",
    "\n",
    "#Print the shape of the query image\n",
    "print('The Query Image has shape:', query_gray.shape)\n",
    "\n",
    "# Print the number of keypoints detected in the training image\n",
    "print(\"\\nNumber of Keypoints Detected In The Training Image: \", len(keypoints_train))\n",
    "\n",
    "# Print the number of keypoints detected in the query image\n",
    "print(\"Number of Keypoints Detected In The Query Image: \", len(keypoints_query))\n",
    "\n",
    "# Print total number of matching points between the training and query images\n",
    "print(\"\\nNumber of Matching Keypoints Between The Training and Query Images: \", len(matches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上面的示例中，请注意训练图像是553 x 471像素，而查询图像是138 x 117像素，是原始训练图像大小的1/4。另外，需要注意的是，查询图像中检测到的关键点数量仅为65，远小于训练图像中的831个关键点。然而，我们可以看到，Brute-Force匹配程序可以将查询图像中的大多数关键点与训练图像中的相应关键点进行匹配。\n",
    "\n",
    "## 旋转不变性\n",
    "\n",
    "ORB算法也具有旋转不变性。这意味着无论方向如何，它都能够检测到图像中的对象。为了看清楚这一点，我们要使用Brute-Force匹配程序来匹配训练图像和旋转了90度的查询图像中的点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the default figure size\n",
    "plt.rcParams['figure.figsize'] = [14.0, 7.0]\n",
    "\n",
    "# Load the training image\n",
    "image1 = cv2.imread('./images/face.jpeg')\n",
    "\n",
    "# Load the query image\n",
    "image2 = cv2.imread('./images/faceR.jpeg')\n",
    "\n",
    "# Convert the training image to RGB\n",
    "training_image = cv2.cvtColor(image1, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Convert the query image to RGB\n",
    "query_image = cv2.cvtColor(image2, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Display the images\n",
    "plt.subplot(121)\n",
    "plt.title('Training Image')\n",
    "plt.imshow(training_image)\n",
    "plt.subplot(122)\n",
    "plt.title('Query Image')\n",
    "plt.imshow(query_image)\n",
    "plt.show()\n",
    "\n",
    "# Convert the training image to gray scale\n",
    "training_gray = cv2.cvtColor(training_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Convert the query image to gray scale\n",
    "query_gray = cv2.cvtColor(query_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Set the parameters of the ORB algorithm by specifying the maximum number of keypoints to locate and\n",
    "# the pyramid decimation ratio\n",
    "orb = cv2.ORB_create(1000, 2.0)\n",
    "\n",
    "# Find the keypoints in the gray scale training and query images and compute their ORB descriptor.\n",
    "# The None parameter is needed to indicate that we are not using a mask in either case.\n",
    "keypoints_train, descriptors_train = orb.detectAndCompute(training_gray, None)\n",
    "keypoints_query, descriptors_query = orb.detectAndCompute(query_gray, None)\n",
    "\n",
    "# Create a Brute Force Matcher object. Set crossCheck to True so that the BFMatcher will only return consistent\n",
    "# pairs. Such technique usually produces best results with minimal number of outliers when there are enough matches.\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck = True)\n",
    "\n",
    "# Perform the matching between the ORB descriptors of the training image and the query image\n",
    "matches = bf.match(descriptors_train, descriptors_query)\n",
    "\n",
    "# The matches with shorter distance are the ones we want. So, we sort the matches according to distance\n",
    "matches = sorted(matches, key = lambda x : x.distance)\n",
    "\n",
    "# Connect the keypoints in the training image with their best matching keypoints in the query image.\n",
    "# The best matches correspond to the first elements in the sorted matches list, since they are the ones\n",
    "# with the shorter distance. We draw the first 100 mathces and use flags = 2 to plot the matching keypoints\n",
    "# without size or orientation.\n",
    "result = cv2.drawMatches(training_gray, keypoints_train, query_gray, keypoints_query, matches[:100], query_gray, flags = 2)\n",
    "\n",
    "# Display the best matching points\n",
    "plt.title('Best Matching Points')\n",
    "plt.imshow(result)\n",
    "plt.show()\n",
    "\n",
    "# Print the number of keypoints detected in the training image\n",
    "print(\"\\nNumber of Keypoints Detected In The Training Image: \", len(keypoints_train))\n",
    "\n",
    "# Print the number of keypoints detected in the query image\n",
    "print(\"Number of Keypoints Detected In The Query Image: \", len(keypoints_query))\n",
    "\n",
    "# Print total number of matching points between the training and query images\n",
    "print(\"\\nNumber of Matching Keypoints Between The Training and Query Images: \", len(matches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上面的例子中，我们看到在两个图像中检测到的关键点的数量非常相似，即使查询图像被旋转了，Brute-Force匹配程序仍然可以对检测到的约78%的关键点进行匹配。此外，请注意大多数匹配关键点都接近特定的人脸特征，例如眼睛、鼻子与嘴巴。\n",
    "\n",
    "## 光照不变性\n",
    "\n",
    "ORB算法也具有光照不变性。这意味着无论光照如何，它都能够检测到图像中的物体。为了看清楚这一点，我们现在将使用Brute-Force匹配程序来匹配训练图像与亮度更亮的查询图像中的点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the default figure size\n",
    "plt.rcParams['figure.figsize'] = [14.0, 7.0]\n",
    "\n",
    "# Load the training image\n",
    "image1 = cv2.imread('./images/face.jpeg')\n",
    "\n",
    "# Load the query image\n",
    "image2 = cv2.imread('./images/faceRI.png')\n",
    "\n",
    "# Convert the training image to RGB\n",
    "training_image = cv2.cvtColor(image1, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Convert the query image to RGB\n",
    "query_image = cv2.cvtColor(image2, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Display the images\n",
    "plt.subplot(121)\n",
    "plt.title('Training Image')\n",
    "plt.imshow(training_image)\n",
    "plt.subplot(122)\n",
    "plt.title('Query Image')\n",
    "plt.imshow(query_image)\n",
    "plt.show()\n",
    "\n",
    "# Convert the training image to gray scale\n",
    "training_gray = cv2.cvtColor(training_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Convert the query image to gray scale\n",
    "query_gray = cv2.cvtColor(query_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Set the parameters of the ORB algorithm by specifying the maximum number of keypoints to locate and\n",
    "# the pyramid decimation ratio\n",
    "orb = cv2.ORB_create(1000, 2.0)\n",
    "\n",
    "# Find the keypoints in the gray scale training and query images and compute their ORB descriptor.\n",
    "# The None parameter is needed to indicate that we are not using a mask in either case.\n",
    "keypoints_train, descriptors_train = orb.detectAndCompute(training_gray, None)\n",
    "keypoints_query, descriptors_query = orb.detectAndCompute(query_gray, None)\n",
    "\n",
    "# Create a Brute Force Matcher object. Set crossCheck to True so that the BFMatcher will only return consistent\n",
    "# pairs. Such technique usually produces best results with minimal number of outliers when there are enough matches.\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck = True)\n",
    "\n",
    "# Perform the matching between the ORB descriptors of the training image and the query image\n",
    "matches = bf.match(descriptors_train, descriptors_query)\n",
    "\n",
    "# The matches with shorter distance are the ones we want. So, we sort the matches according to distance\n",
    "matches = sorted(matches, key = lambda x : x.distance)\n",
    "\n",
    "# Connect the keypoints in the training image with their best matching keypoints in the query image.\n",
    "# The best matches correspond to the first elements in the sorted matches list, since they are the ones\n",
    "# with the shorter distance. We draw the first 100 mathces and use flags = 2 to plot the matching keypoints\n",
    "# without size or orientation.\n",
    "result = cv2.drawMatches(training_gray, keypoints_train, query_gray, keypoints_query, matches[:100], query_gray, flags = 2)\n",
    "\n",
    "# Display the best matching points\n",
    "plt.title('Best Matching Points')\n",
    "plt.imshow(result)\n",
    "plt.show()\n",
    "\n",
    "# Print the number of keypoints detected in the training image\n",
    "print(\"\\nNumber of Keypoints Detected In The Training Image: \", len(keypoints_train))\n",
    "\n",
    "# Print the number of keypoints detected in the query image\n",
    "print(\"Number of Keypoints Detected In The Query Image: \", len(keypoints_query))\n",
    "\n",
    "# Print total number of matching points between the training and query images\n",
    "print(\"\\nNumber of Matching Keypoints Between The Training and Query Images: \", len(matches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上面的例子中，我们看到在两个图像中检测到的关键点的数量依然非常相似，即使查询图像的亮度更亮，Brute-Force匹配程序仍然可以对检测到的约63％的关键点进行匹配。\n",
    "\n",
    "## 抗噪性\n",
    "\n",
    "ORB算法也具有抗噪性。这意味着即使图像具有一定程度的噪声，它也能够检测到图像中的对象。为了看清楚这一点，我们现在将使用Brute-Force匹配程序来匹配训练图像和具有大量噪声的查询图像中的点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the default figure size\n",
    "plt.rcParams['figure.figsize'] = [14.0, 7.0]\n",
    "\n",
    "# Load the training image\n",
    "image1 = cv2.imread('./images/face.jpeg')\n",
    "\n",
    "# Load the noisy, gray scale query image. \n",
    "image2 = cv2.imread('./images/faceRN5.png')\n",
    "\n",
    "# Convert the query image to gray scale\n",
    "query_gray = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Convert the training image to gray scale\n",
    "training_gray = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Display the images\n",
    "plt.subplot(121)\n",
    "plt.imshow(training_gray, cmap = 'gray')\n",
    "plt.title('Gray Scale Training Image')\n",
    "plt.subplot(122)\n",
    "plt.imshow(query_gray, cmap = 'gray')\n",
    "plt.title('Query Image')\n",
    "plt.show()\n",
    "\n",
    "# Set the parameters of the ORB algorithm by specifying the maximum number of keypoints to locate and\n",
    "# the pyramid decimation ratio\n",
    "orb = cv2.ORB_create(1000, 1.3)\n",
    "\n",
    "# Find the keypoints in the gray scale training and query images and compute their ORB descriptor.\n",
    "# The None parameter is needed to indicate that we are not using a mask in either case. \n",
    "keypoints_train, descriptors_train = orb.detectAndCompute(training_gray, None)\n",
    "keypoints_query, descriptors_query = orb.detectAndCompute(query_gray, None)\n",
    "\n",
    "# Create a Brute Force Matcher object. We set crossCheck to True so that the BFMatcher will only return consistent\n",
    "# pairs. Such technique usually produces best results with minimal number of outliers when there are enough matches.\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck = True)\n",
    "\n",
    "# Perform the matching between the ORB descriptors of the training image and the query image\n",
    "matches = bf.match(descriptors_train, descriptors_query)\n",
    "\n",
    "# The matches with shorter distance are the ones we want. So, we sort the matches according to distance\n",
    "matches = sorted(matches, key = lambda x : x.distance)\n",
    "\n",
    "# Connect the keypoints in the training image with their best matching keypoints in the query image.\n",
    "# The best matches correspond to the first elements in the sorted matches list, since they are the ones\n",
    "# with the shorter distance. We draw the first 100 mathces and use flags = 2 to plot the matching keypoints\n",
    "# without size or orientation.\n",
    "result = cv2.drawMatches(training_gray, keypoints_train, query_gray, keypoints_query, matches[:100], query_gray, flags = 2)\n",
    "\n",
    "# we display the image\n",
    "plt.title('Best Matching Points')\n",
    "plt.imshow(result)\n",
    "plt.show()\n",
    "\n",
    "# Print the number of keypoints detected in the training image\n",
    "print(\"Number of Keypoints Detected In The Training Image: \", len(keypoints_train))\n",
    "\n",
    "# Print the number of keypoints detected in the query image\n",
    "print(\"Number of Keypoints Detected In The Query Image: \", len(keypoints_query))\n",
    "\n",
    "# Print total number of matching points between the training and query images\n",
    "print(\"\\nNumber of Matching Keypoints Between The Training and Query Images: \", len(matches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上面的例子中，我们同样看到了两个图像中检测到的关键点的数量非常相似，即使查询图像有很多噪声，Brute-Force匹配程序仍然可以对检测到的大约63％的关键点进行匹配。此外，请注意，大多数匹配关键点都接近特定的人脸特征，例如眼睛、鼻子与嘴巴。此外，我们可以看到有一些特征不太匹配，这可能是因为图像区域中的强度模式相似而被选中。另外，还要说明的一点是，在这个示例中，我们使用的金字塔抽取比率为1.3，而不是前面示例中使用的值2.0，因为在这个特殊的例子中，这个比率会产生更好的结果。\n",
    "\n",
    "\n",
    "# 物体检测\n",
    "\n",
    "我们现在将运用ORB算法，实现在另一图像中检测训练图像中的人脸。与之前一样，首先需要加载训练和查询图像。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the default figure size\n",
    "plt.rcParams['figure.figsize'] = [14.0, 7.0]\n",
    "\n",
    "# Load the training image\n",
    "image1 = cv2.imread('./images/face.jpeg')\n",
    "\n",
    "# Load the query image\n",
    "image2 = cv2.imread('./images/Team.jpeg')\n",
    "\n",
    "# Convert the training image to RGB\n",
    "training_image = cv2.cvtColor(image1, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Convert the query image to RGB\n",
    "query_image = cv2.cvtColor(image2, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Display the images\n",
    "plt.subplot(121)\n",
    "plt.imshow(training_image)\n",
    "plt.title('Training Image')\n",
    "plt.subplot(122)\n",
    "plt.imshow(query_image)\n",
    "plt.title('Query Image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这个特定的例子中，训练图像中包含一个人脸，因此，于我们之前所见相同，检测到的大多数关键点都接近人脸特征，例如眼睛、鼻子与嘴巴。另一方面，我们的查询图像是一组人的图片，其中一个是我们想要检测的女人。接下来要做的是，检测查询图像的关键点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the default figure size\n",
    "plt.rcParams['figure.figsize'] = [34.0, 34.0]\n",
    "\n",
    "# Convert the training image to gray scale\n",
    "training_gray = cv2.cvtColor(training_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Convert the query image to gray scale\n",
    "query_gray = cv2.cvtColor(query_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Set the parameters of the ORB algorithm by specifying the maximum number of keypoints to locate and\n",
    "# the pyramid decimation ratio\n",
    "orb = cv2.ORB_create(5000, 2.0)\n",
    "\n",
    "# Find the keypoints in the gray scale training and query images and compute their ORB descriptor.\n",
    "# The None parameter is needed to indicate that we are not using a mask in either case.  \n",
    "keypoints_train, descriptors_train = orb.detectAndCompute(training_gray, None)\n",
    "keypoints_query, descriptors_query = orb.detectAndCompute(query_gray, None)\n",
    "\n",
    "# Create copies of the query images to draw our keypoints on\n",
    "query_img_keyp = copy.copy(query_image)\n",
    "\n",
    "# Draw the keypoints with size and orientation on the copy of the query image\n",
    "cv2.drawKeypoints(query_image, keypoints_query, query_img_keyp, flags = cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "# Display the query image with the keypoints with size and orientation\n",
    "plt.title('Keypoints With Size and Orientation', fontsize = 30)\n",
    "plt.imshow(query_img_keyp)\n",
    "plt.show()\n",
    "\n",
    "# Print the number of keypoints detected\n",
    "print(\"\\nNumber of keypoints Detected: \", len(keypoints_query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以看到，该查询图像在图像的许多部分都有关键点。现在，已经获得了训练图像和查询图像的关键点与ORB描述子，接下来，就可以使用Brute-Force匹配程序来尝试在查询图像中寻找这位女性的脸部。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the default figure size\n",
    "plt.rcParams['figure.figsize'] = [34.0, 34.0]\n",
    "\n",
    "# Create a Brute Force Matcher object. We set crossCheck to True so that the BFMatcher will only return consistent\n",
    "# pairs. Such technique usually produces best results with minimal number of outliers when there are enough matches.\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck = True)\n",
    "\n",
    "# Perform the matching between the ORB descriptors of the training image and the query image\n",
    "matches = bf.match(descriptors_train, descriptors_query)\n",
    "\n",
    "# The matches with shorter distance are the ones we want. So, we sort the matches according to distance\n",
    "matches = sorted(matches, key = lambda x : x.distance)\n",
    "\n",
    "# Connect the keypoints in the training image with their best matching keypoints in the query image.\n",
    "# The best matches correspond to the first elements in the sorted matches list, since they are the ones\n",
    "# with the shorter distance. We draw the first 85 mathces and use flags = 2 to plot the matching keypoints\n",
    "# without size or orientation.\n",
    "result = cv2.drawMatches(training_gray, keypoints_train, query_gray, keypoints_query, matches[:85], query_gray, flags = 2)\n",
    "\n",
    "# we display the image\n",
    "plt.title('Best Matching Points', fontsize = 30)\n",
    "plt.imshow(result)\n",
    "plt.show()\n",
    "\n",
    "# Print the number of keypoints detected in the training image\n",
    "print(\"Number of Keypoints Detected In The Training Image: \", len(keypoints_train))\n",
    "\n",
    "# Print the number of keypoints detected in the query image\n",
    "print(\"Number of Keypoints Detected In The Query Image: \", len(keypoints_query))\n",
    "\n",
    "# Print total number of matching Keypoints between the training and query images\n",
    "print(\"\\nNumber of Matching Keypoints Between The Training and Query Images: \", len(matches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以清楚地看到，即使查询图像中有许多人脸和对象，Brute-Force匹配程序也能够在查询图像中正确地定位到该女性的脸部。"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
